{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama Server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Ollama and Web Server tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n7XGkmKXTat"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!wget -P ~ https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i ~/cloudflared-linux-amd64.deb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nohup ollama serve > ollama.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ollama pull llava\n",
        "!ollama pull llama3.2:1b\n",
        "!ollama pull deepseek-r1:14b\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Web Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nohup cloudflared tunnel --url http://localhost:11434 --http-host-header=\"localhost:11434\" > cloud.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!grep trycloud cloud.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxMI8t_iqwkJ"
      },
      "source": [
        "# Prompting with Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, base64\n",
        "\n",
        "from ollama import Client\n",
        "\n",
        "OLLAMA_URL = \"http://127.0.0.1:11434\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client(host=OLLAMA_URL)\n",
        "\n",
        "response = client.chat(\n",
        "    model='llama3.2:1b',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'Why are eggs yellow?',\n",
        "    }]\n",
        ")\n",
        "\n",
        "# display(response)\n",
        "display(response[\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_data = requests.get(\"https://mystickermania.com/cdn/stickers/gudetama/gudetama-strawberry-512x512.png\").content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"../imgs/landscape_00.jpg\", \"rb\") as ifp:\n",
        "  img_data = ifp.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client(host=OLLAMA_URL)\n",
        "\n",
        "img = base64.b64encode(img_data).decode()\n",
        "\n",
        "response = client.chat(\n",
        "    model='llava',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'What is in this image? Be concise.',\n",
        "        'images': [img],\n",
        "    }]\n",
        ")\n",
        "\n",
        "# display(response)\n",
        "display(response[\"message\"][\"content\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
